You are an expert-level software architect and Python developer. Your task is to design and build a complete, high-concurrency data generation pipeline.Core Objective:Build a "super lightweight yet ultra-fast" system to synthetically generate a large (5000+) dataset of AI training data in jsonl format for axolotl. The system must use a two-step LLM call process via the OpenRouter API:Generate: Call a fast LLM (e.g., Mistral 7B) to generate a data record.Refine: Call a powerful LLM (e.g., GPT-4o) to proofread, improve, and format the record.The data's domain is crypto trading, intended to fine-tune a model to become an expert trader.Key Performance Requirement:The system must be architected for extreme I/O-bound concurrency to achieve a target of 100+ parallel API requests per second to OpenRouter.System Architecture & Technology Stack:You must use the following lightweight, high-performance stack:Language: Python 3.10+ (using asyncio for concurrency)Queue System: Redis (used as a simple, fast job queue)API/Control Panel: FastAPI (for its async capabilities and speed)HTTP Client: aiohttp (for high-performance async HTTP requests with connection pooling)Deployment: docker-compose.ymlDetailed Component Breakdown:1. docker-compose.yml File:Create a docker-compose.yml file that defines three services:redis: Use the official redis:alpine image.api: The FastAPI control panel. Builds from the local directory.worker: The Python asyncio worker. Builds from the local directory. This service must be easily scalable (e.t., docker-compose up --scale worker=10).2. api.py (FastAPI Control Panel):This file provides a simple web UI and API to control the pipeline.It must use FastAPI and uvicorn.It must connect to the redis service.Web UI (HTML):Serve a single, simple HTML page.Controls:Input field: "Number of Data Rows to Generate" (e.g., 5000).Text area: "Base Prompt for Generator LLM" (pre-fill with an example).Buttons: "START", "PAUSE", "RESUME".Real-time Stats:"State": (e.g., STOPPED, RUNNING, PAUSED)"Jobs in Queue": (Get from LLEN job_queue)"Jobs Processed": (Get from GET processed_count)"Errors": (Get from LLEN job_errors)Download: A link to /download to get the final output.jsonl.API Endpoints:/: Serves the HTML UI./start: Takes the number of jobs and base prompt. Clears old data (processed_count, job_errors, output.jsonl). Pushes N jobs (e.g., {"id": 1, "base_prompt": "..."}) into the job_queue list in Redis. Sets a Redis key pipeline_state to "running"./pause: Sets pipeline_state to "paused"./resume: Sets pipeline_state to "running"./stats: (For the UI to poll) Returns a JSON object with the real-time stats./download: Serves the output.jsonl file for download.3. worker.py (High-Concurrency Worker):This is the most critical component.It must use asyncio, aioredis (async redis client), and aiohttp.On startup, it creates a single aiohttp.ClientSession to be reused for all API calls (for connection pooling).It must read the OPENROUTER_API_KEY from an environment variable.It runs a main async loop:Check the pipeline_state key in Redis. If it's "paused" or not "running", await asyncio.sleep(2) and check again.Use asyncio.Semaphore(100) to limit concurrent tasks to 100 (matching the 100 req/s target).If "running", await semaphore.acquire() and then try to BRPOP a job from the job_queue.When a job is received, asyncio.create_task(process_job(job)) to run it concurrently. The task must release the semaphore when it's done (e.g., in a finally block).async def process_job(job): Function:Handle Errors: Wrap the entire logic in a try...except block.Step 1: Generate Data:Use the aiohttp.ClientSession to POST to OpenRouter's API.Model: Use a fast model (e.g., mistralai/mistral-7b-instruct:free).Prompt: Use the base_prompt from the job. Example: "You are a crypto trading expert. Create a realistic, complex trading scenario. Include market data, news, and technical analysis. Then, ask a difficult question about the best action to take (Buy, Sell, Hold) and why."Step 2: Refine Data:Take the text output from Step 1.Use the aiohttp.ClientSession to POST to OpenRouter's API again.Model: Use a powerful model (e.g., openai/gpt-4o).Prompt: "You are an world-class trading analyst and proofreader. The following text contains a trading scenario and a question. Your task is to: 1. Correct any errors. 2. Improve the quality, sharpness, and realism of the analysis. 3. Format the entire result only as a perfect JSON object for finetuning, using this exact structure: {\"instruction\": \"...\", \"input\": \"...\", \"output\": \"...\"}. The 'instruction' should be the question, 'input' should be the market/scenario context, and 'output' should be the expert answer. Here is the text: [INSERT LLM 1 OUTPUT HERE]"Parse & Save:The output from Step 2 should be a JSON string. Parse it.If parsing fails, log the bad output to the job_errors list in Redis and return.Use asyncio.Lock to get exclusive access to the output file.Append the parsed JSON object as a single line to output.jsonl (using json.dumps).Update Stats: INCR processed_count in Redis.Error Handling (inside except): If any API call or step fails (e.g., rate limit, timeout), log the error and the job to the job_errors list in Redis. Implement a simple exponential backoff retry (e.g., 3 retries) for API calls.4. requirements.txt File:List all necessary Python packages:fastapi
uvicorn[standard]
python-multipart
jinja2
redis
aioredis
aiohttp
asyncio
